[
{
	"uri": "/1_setup/login_aws_account.html",
	"title": "1.1 Login to your temporary workshop AWS Account",
	"tags": [],
	"description": "",
	"content": " Get your temporary AWS account Click on the link at the bottom of the browser as show below.\nClick on Accept Terms \u0026amp; Login Click on Email One-Time OTP (Allow for up to 2 mins to receive the passcode) Provide your email address Enter your OTP code Click on AWS Console Click on Open AWS Console In the AWS Console click on Amazon SageMaker Click on Amazon SageMaker Studio and then click on Open Studio You should now have Amazon SageMaker Studio interface open on your browser "
},
{
	"uri": "/3_build-train-tune-deploy/finetune_huggingface.html",
	"title": "3.1 Train and fine-tune NLP models with SageMaker and HuggingFace library",
	"tags": [],
	"description": "",
	"content": " Watch the livestream to follow along with the presenter\n Open the following notebook to follow along Notebook: finetuning_huggingface.ipynb\nA copy of the code from the notebook is also available below, if you prefer building your notebook from scratch by copy pasting each code cell and then running them.\n Upgrade to the latest sagemaker version.\n!pip install -Uq sagemaker smdebugimport ast import json import boto3 import numpy as np import pandas as pd from datetime import datetime as dt from IPython.display import FileLink import sagemaker from sagemaker import TrainingJobAnalytics from sagemaker.debugger import Rule, ProfilerRule, rule_configs from sagemaker.debugger import ProfilerConfig, FrameworkProfile, DebuggerHookConfig from sagemaker.huggingface import HuggingFace, HuggingFaceModel, HuggingFacePredictor from smdebug.profiler.analysis.notebook_utils.training_job import TrainingJob from smdebug.profiler.analysis.notebook_utils.timeline_charts import TimelineCharts from sklearn.metrics import classification_report# permissions sess = sagemaker.Session() role = sagemaker.get_execution_role() s3_client = boto3.client(\u0026#39;s3\u0026#39;) bucket = sess.default_bucket() prefix = \u0026#34;huggingface_classifier\u0026#34; sess = sagemaker.Session(default_bucket=bucket) print(f\u0026#34;sagemaker role arn: {role}\u0026#34;) print(f\u0026#34;sagemaker bucket: {sess.default_bucket()}\u0026#34;) print(f\u0026#34;sagemaker session region: {sess.boto_region_name}\u0026#34;) Downloading the Dataset df = pd.read_csv(\u0026#39;./data/Womens Clothing E-Commerce Reviews.csv\u0026#39;) df = df[[\u0026#39;Review Text\u0026#39;,\t\u0026#39;Rating\u0026#39;]] df.columns = [\u0026#39;text\u0026#39;, \u0026#39;label\u0026#39;] df[\u0026#39;label\u0026#39;] = df[\u0026#39;label\u0026#39;] - 1 df = df.dropna() train, validate, test = \\ np.split(df.sample(frac=1, random_state=42), [int(.6*len(df)), int(.8*len(df))]) train.shape, validate.shape, test.shapetrain.to_csv(\u0026#39;./data/train.csv\u0026#39;, index=False) validate.to_csv(\u0026#39;./data/validate.csv\u0026#39;, index=False) test.to_csv(\u0026#39;./data/test.csv\u0026#39;, index=False)s3_client.upload_file(\u0026#39;./data/train.csv\u0026#39;, bucket, f\u0026#39;{prefix}/data/train.csv\u0026#39;) s3_client.upload_file(\u0026#39;./data/validate.csv\u0026#39;, bucket, f\u0026#39;{prefix}/data/validate.csv\u0026#39;) s3_client.upload_file(\u0026#39;./data/test.csv\u0026#39;, bucket, f\u0026#39;{prefix}/data/test.csv\u0026#39;) Prepare a HuggingFace Transformers fine-tuning script. !mkdir ./src%%writefile src/train.py import os import sys import logging import argparse from datasets import load_dataset from sklearn.metrics import accuracy_score, precision_recall_fscore_support from transformers.trainer_utils import get_last_checkpoint from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer # Set up logging logger = logging.getLogger(__name__) logging.basicConfig( level=logging.getLevelName(\u0026#34;INFO\u0026#34;), handlers=[logging.StreamHandler(sys.stdout)], format=\u0026#34;%(asctime)s- %(name)s- %(levelname)s- %(message)s\u0026#34;, ) if __name__ == \u0026#34;__main__\u0026#34;: logger.info(sys.argv) parser = argparse.ArgumentParser() # hyperparameters sent by the client are passed as command-line arguments to the script. parser.add_argument(\u0026#34;--epochs\u0026#34;, type=int, default=3) parser.add_argument(\u0026#34;--train-batch-size\u0026#34;, type=int, default=32) parser.add_argument(\u0026#34;--eval-batch-size\u0026#34;, type=int, default=64) parser.add_argument(\u0026#34;--warmup_steps\u0026#34;, type=int, default=500) parser.add_argument(\u0026#34;--model_name\u0026#34;, type=str) parser.add_argument(\u0026#34;--learning_rate\u0026#34;, type=str, default=5e-5) parser.add_argument(\u0026#34;--output_dir\u0026#34;, type=str) # Data, model, and output directories parser.add_argument(\u0026#34;--output-data-dir\u0026#34;, type=str, default=os.environ[\u0026#34;SM_OUTPUT_DATA_DIR\u0026#34;]) parser.add_argument(\u0026#34;--model-dir\u0026#34;, type=str, default=os.environ[\u0026#34;SM_MODEL_DIR\u0026#34;]) parser.add_argument(\u0026#34;--n_gpus\u0026#34;, type=str, default=os.environ[\u0026#34;SM_NUM_GPUS\u0026#34;]) parser.add_argument(\u0026#34;--training_dir\u0026#34;, type=str, default=os.environ[\u0026#34;SM_CHANNEL_TRAIN\u0026#34;]) parser.add_argument(\u0026#34;--test_dir\u0026#34;, type=str, default=os.environ[\u0026#34;SM_CHANNEL_TEST\u0026#34;]) args, _ = parser.parse_known_args() # Set up logging logger = logging.getLogger(__name__) logging.basicConfig( level=logging.getLevelName(\u0026#34;INFO\u0026#34;), handlers=[logging.StreamHandler(sys.stdout)], format=\u0026#34;%(asctime)s- %(name)s- %(levelname)s- %(message)s\u0026#34;, ) # download tokenizer tokenizer = AutoTokenizer.from_pretrained(args.model_name) # Load dataset train_file = f\u0026#34;{args.training_dir}/train.csv\u0026#34; validate_file = f\u0026#34;{args.test_dir}/validate.csv\u0026#34; dataset = load_dataset(\u0026#39;csv\u0026#39;, data_files={\u0026#39;train\u0026#39;: train_file, \u0026#39;test\u0026#39;: validate_file}) train_dataset = dataset[\u0026#39;train\u0026#39;] test_dataset = dataset[\u0026#39;test\u0026#39;] logger.info(f\u0026#34; loaded train_dataset length is: {len(train_dataset)}\u0026#34;) logger.info(f\u0026#34; loaded test_dataset length is: {len(test_dataset)}\u0026#34;) # tokenizer helper function def tokenize(batch): return tokenizer(batch[\u0026#39;text\u0026#39;], padding=\u0026#39;max_length\u0026#39;, truncation=True) # tokenize dataset train_dataset = train_dataset.map(tokenize, batched=True) test_dataset = test_dataset.map(tokenize, batched=True) # set format for pytorch train_dataset = train_dataset.rename_column(\u0026#34;label\u0026#34;, \u0026#34;labels\u0026#34;) train_dataset.set_format(\u0026#39;torch\u0026#39;, columns=[\u0026#39;input_ids\u0026#39;, \u0026#39;attention_mask\u0026#39;, \u0026#39;labels\u0026#39;]) test_dataset = test_dataset.rename_column(\u0026#34;label\u0026#34;, \u0026#34;labels\u0026#34;) test_dataset.set_format(\u0026#39;torch\u0026#39;, columns=[\u0026#39;input_ids\u0026#39;, \u0026#39;attention_mask\u0026#39;, \u0026#39;labels\u0026#39;]) logger.info(f\u0026#34; loaded train_dataset length is: {len(train_dataset)}\u0026#34;) logger.info(f\u0026#34; loaded test_dataset length is: {len(test_dataset)}\u0026#34;) # compute metrics function for binary classification def compute_metrics(pred): labels = pred.label_ids preds = pred.predictions.argmax(-1) precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\u0026#34;weighted\u0026#34;) acc = accuracy_score(labels, preds) return {\u0026#34;accuracy\u0026#34;: acc, \u0026#34;f1\u0026#34;: f1, \u0026#34;precision\u0026#34;: precision, \u0026#34;recall\u0026#34;: recall} # download model from model hub model = AutoModelForSequenceClassification.from_pretrained(args.model_name, num_labels=5) # define training args training_args = TrainingArguments( output_dir=args.output_dir, num_train_epochs=args.epochs, per_device_train_batch_size=args.train_batch_size, per_device_eval_batch_size=args.eval_batch_size, warmup_steps=args.warmup_steps, evaluation_strategy=\u0026#34;epoch\u0026#34;, logging_dir=f\u0026#34;{args.output_data_dir}/logs\u0026#34;, learning_rate=float(args.learning_rate), ) # create Trainer instance trainer = Trainer( model=model, args=training_args, compute_metrics=compute_metrics, train_dataset=train_dataset, eval_dataset=test_dataset, ) # train model if get_last_checkpoint(args.output_dir) is not None: logger.info(\u0026#34;***** continue training *****\u0026#34;) trainer.train(resume_from_checkpoint=args.output_dir) else: trainer.train() # evaluate model eval_result = trainer.evaluate(eval_dataset=test_dataset) # writes eval result to file which can be accessed later in s3 ouput with open(os.path.join(args.output_data_dir, \u0026#34;eval_results.txt\u0026#34;), \u0026#34;w\u0026#34;) as writer: print(f\u0026#34;***** Eval results *****\u0026#34;) for key, value in sorted(eval_result.items()): writer.write(f\u0026#34;{key} = {value}\\n\u0026#34;) # Saves the model to s3 trainer.save_model(args.model_dir) tokenizer.save_pretrained(args.model_dir) Create an HuggingFace Estimator # hyperparameters, which are passed into the training job hyperparameters={\u0026#39;epochs\u0026#39;: 1, \u0026#39;train_batch_size\u0026#39;: 32, \u0026#39;model_name\u0026#39;:\u0026#39;distilbert-base-uncased\u0026#39;, \u0026#39;output_dir\u0026#39;:\u0026#39;/opt/ml/checkpoints\u0026#39; } metric_definitions=[ {\u0026#39;Name\u0026#39;: \u0026#39;loss\u0026#39;, \u0026#39;Regex\u0026#39;: \u0026#34;\u0026#39;loss\u0026#39;: ([0-9]+(.|e\\-)[0-9]+),?\u0026#34;}, {\u0026#39;Name\u0026#39;: \u0026#39;learning_rate\u0026#39;, \u0026#39;Regex\u0026#39;: \u0026#34;\u0026#39;learning_rate\u0026#39;: ([0-9]+(.|e\\-)[0-9]+),?\u0026#34;}, {\u0026#39;Name\u0026#39;: \u0026#39;eval_loss\u0026#39;, \u0026#39;Regex\u0026#39;: \u0026#34;\u0026#39;eval_loss\u0026#39;: ([0-9]+(.|e\\-)[0-9]+),?\u0026#34;}, {\u0026#39;Name\u0026#39;: \u0026#39;eval_accuracy\u0026#39;, \u0026#39;Regex\u0026#39;: \u0026#34;\u0026#39;eval_accuracy\u0026#39;: ([0-9]+(.|e\\-)[0-9]+),?\u0026#34;}, {\u0026#39;Name\u0026#39;: \u0026#39;eval_f1\u0026#39;, \u0026#39;Regex\u0026#39;: \u0026#34;\u0026#39;eval_f1\u0026#39;: ([0-9]+(.|e\\-)[0-9]+),?\u0026#34;}, {\u0026#39;Name\u0026#39;: \u0026#39;eval_precision\u0026#39;, \u0026#39;Regex\u0026#39;: \u0026#34;\u0026#39;eval_precision\u0026#39;: ([0-9]+(.|e\\-)[0-9]+),?\u0026#34;}, {\u0026#39;Name\u0026#39;: \u0026#39;eval_recall\u0026#39;, \u0026#39;Regex\u0026#39;: \u0026#34;\u0026#39;eval_recall\u0026#39;: ([0-9]+(.|e\\-)[0-9]+),?\u0026#34;}, {\u0026#39;Name\u0026#39;: \u0026#39;eval_runtime\u0026#39;, \u0026#39;Regex\u0026#39;: \u0026#34;\u0026#39;eval_runtime\u0026#39;: ([0-9]+(.|e\\-)[0-9]+),?\u0026#34;}, {\u0026#39;Name\u0026#39;: \u0026#39;eval_samples_per_second\u0026#39;, \u0026#39;Regex\u0026#39;: \u0026#34;\u0026#39;eval_samples_per_second\u0026#39;: ([0-9]+(.|e\\-)[0-9]+),?\u0026#34;}, {\u0026#39;Name\u0026#39;: \u0026#39;epoch\u0026#39;, \u0026#39;Regex\u0026#39;: \u0026#34;\u0026#39;epoch\u0026#39;: ([0-9]+(.|e\\-)[0-9]+),?\u0026#34;}] Configure rules We specify the following rules:\n loss_not_decreasing: checks if loss is decreasing and triggers if the loss has not decreased by a certain persentage in the last few iterations LowGPUUtilization: checks if GPU is under-utilizated ProfilerReport: runs the entire set of performance rules and create a final output report with further insights and recommendations.\n# Configure a Profiler rule object rules = [ Rule.sagemaker(rule_configs.loss_not_decreasing()), ProfilerRule.sagemaker(rule_configs.LowGPUUtilization()), ProfilerRule.sagemaker(rule_configs.ProfilerReport()) ]  The following configuration will capture system metrics at 500 milliseconds. The system metrics include utilization per CPU, GPU, memory utilization per CPU, GPU as well I/O and network.\nDebugger will capture detailed profiling information from step 5 to step 15. This information includes Horovod metrics, dataloading, preprocessing, operators running on CPU and GPU.\n# Specify a profiler configuration profiler_config = ProfilerConfig( system_monitor_interval_millis=500, framework_profile_params=FrameworkProfile(num_steps=10) )# s3 uri where our checkpoints will be uploaded during training job_name = \u0026#34;using-spot\u0026#34; checkpoint_s3_uri = f\u0026#39;s3://{bucket}/{prefix}/{job_name}/checkpoints\u0026#39; # create the Estimator huggingface_estimator = HuggingFace(entry_point=\u0026#39;train.py\u0026#39;, source_dir=\u0026#39;./src\u0026#39;, instance_type=\u0026#39;ml.p3.2xlarge\u0026#39;, size=5, instance_count=1, base_job_name=job_name, checkpoint_s3_uri=checkpoint_s3_uri, # use_spot_instances=True, # max_wait=3600, # This should be equal to or greater than max_run in seconds\u0026#39; # max_run=1000, # expected max run in seconds role=role, transformers_version=\u0026#39;4.6\u0026#39;, pytorch_version=\u0026#39;1.7\u0026#39;, py_version=\u0026#39;py36\u0026#39;, hyperparameters = hyperparameters, metric_definitions=metric_definitions, # Debugger-specific parameters profiler_config=profiler_config, debugger_hook_config=hook_config, rules=rules, ) Excute the fine-tuning Job data = {\u0026#39;train\u0026#39;: f\u0026#34;s3://{bucket}/{prefix}/data/train.csv\u0026#34;, \u0026#39;test\u0026#39;: f\u0026#34;s3://{bucket}/{prefix}/data/validate.csv\u0026#34; } huggingface_estimator.fit(data, wait=True) Accessing Training Metrics # Captured metrics can be accessed as a Pandas dataframe training_job_name = huggingface_estimator.latest_training_job.name print(f\u0026#34;Training jobname: {training_job_name}\u0026#34;) df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe() df.head(10) Analyze Profiling Data While the training is still in progress you can visualize the performance data in SageMaker Studio or in the notebook. Debugger provides utilities to plot system metrics in form of timeline charts or heatmaps. Checkout out the notebook profiling_interactive_analysis.ipynb for more details. In the following code cell we plot the total CPU and GPU utilization as timeseries charts. To visualize other metrics such as I/O, memory, network you simply need to extend the list passed to select_dimension and select_events.\nsession = boto3.session.Session() region = session.region_name tj = TrainingJob(training_job_name, region) tj.wait_for_sys_profiling_data_to_be_available()system_metrics_reader = tj.get_systems_metrics_reader() system_metrics_reader.refresh_event_file_list() view_timeline_charts = TimelineCharts( system_metrics_reader, framework_metrics_reader=None, select_dimensions=[\u0026#34;CPU\u0026#34;, \u0026#34;GPU\u0026#34;], select_events=[\u0026#34;total\u0026#34;], ) Download Debugger Profling Report The profiling report rule will create an html report profiler-report.html with a summary of builtin rules and recommenades of next steps. You can find this report in your S3 bucket.\nrule_output_path = huggingface_estimator.output_path + huggingface_estimator.latest_training_job.job_name + \u0026#34;/rule-output\u0026#34; print(f\u0026#34;You will find the profiler report in {rule_output_path}\u0026#34;)s3_client.download_file(Bucket=bucket, Key=f\u0026#39;using-spot-2021-07-23-17-16-24-982/rule-output/ProfilerReport/profiler-output/profiler-report.html\u0026#39;, Filename=\u0026#39;./debugger_report.html\u0026#39;) display(\u0026#34;Click link below to view the debugger repot.\u0026#34;, FileLink(\u0026#34;./debugger_report.html\u0026#34;)) For more information about how to download and open the Debugger profiling report, see SageMaker Debugger Profiling Report in the SageMaker developer guide.\nDeploying the endpoint endpoint_name = f\u0026#39;huggingface-finetune-{dt.today().strftime(\u0026#39;%Y-%m-%d\u0026#39;)}\u0026#39; # create Hugging Face Model Class huggingface_model = HuggingFaceModel( model_data=huggingface_estimator.model_data, # S3 path to your trained sagemaker model role=role, # IAM role with permissions to create an Endpoint transformers_version=\u0026#39;4.6\u0026#39;, pytorch_version=\u0026#39;1.7\u0026#39;, py_version=\u0026#39;py36\u0026#39; ) # deploy model to SageMaker Inference predictor = huggingface_model.deploy( initial_instance_count=1, instance_type=\u0026#34;ml.m5.xlarge\u0026#34;, endpoint_name = endpoint_name ) Evaluate predictions on the test set pred_list = [] for idx, row in test.head().iterrows(): payload = {\u0026#34;inputs\u0026#34;: row[\u0026#39;text\u0026#39;]} pred = predictor.predict(payload)[0] # rename label to prediction pred[\u0026#39;prediction\u0026#39;] = pred.pop(\u0026#39;label\u0026#39;) # convert prediction value to int pred[\u0026#39;prediction\u0026#39;] = int(pred[\u0026#39;prediction\u0026#39;].replace(\u0026#39;LABEL_\u0026#39;, \u0026#39;\u0026#39;)) pred_list.append(pred)test[\u0026#39;prediction\u0026#39;] = pred_list df_test = pd.concat([test.drop([\u0026#39;prediction\u0026#39;], axis=1), test[\u0026#39;prediction\u0026#39;].apply(pd.Series)], axis=1)print(classification_report(df_test[\u0026#39;label\u0026#39;], df_test[\u0026#39;prediction\u0026#39;])) Invoke the endpoint with the Python SDK # client = boto3.client(\u0026#39;sagemaker\u0026#39;) # endpoint = client.list_endpoints()[\u0026#39;Endpoints\u0026#39;]payload = {\u0026#34;inputs\u0026#34;: [test[\u0026#39;text\u0026#39;].iloc[0]]} predictor = HuggingFacePredictor(endpoint_name=endpoint_name, sagemaker_session=sess ) result = predictor.predict(data=payload)[0] print(f\u0026#34;Predicted \\033[1m{result[\u0026#39;label\u0026#39;]}\\033[0m with score of \\033[1m{round(result[\u0026#39;score\u0026#39;], 2)}\\033[0m. Real label is \\033[1m{test[\u0026#39;label\u0026#39;].iloc[0]}\\033[0m. Full sentence:\\n\\n{test[\u0026#39;text\u0026#39;].iloc[0]}\u0026#34;) Alternative: invoke the endpoint with boto3 client = boto3.client(\u0026#39;sagemaker-runtime\u0026#39;) payload = {\u0026#34;inputs\u0026#34;: [test[\u0026#39;text\u0026#39;].iloc[0]]} user_encode_data = json.dumps(payload).encode(\u0026#39;utf-8\u0026#39;) response = client.invoke_endpoint(EndpointName=endpoint_name, Body=user_encode_data, ContentType=\u0026#39;application/json\u0026#39; ) result = ast.literal_eval(response[\u0026#39;Body\u0026#39;].read().decode())[0] print(f\u0026#34;Predicted \\033[1m{result[\u0026#39;label\u0026#39;]}\\033[0m with score of \\033[1m{round(result[\u0026#39;score\u0026#39;], 2)}\\033[0m. Real label is \\033[1m{test[\u0026#39;label\u0026#39;].iloc[0]}\\033[0m. Full sentence:\\n\\n{test[\u0026#39;text\u0026#39;].iloc[0]}\u0026#34;) Clean up Make sure you delete the SageMaker endpoints to avoid unnecessary costs:\n# predictor.delete_endpoint()"
},
{
	"uri": "/appendix/docs.html",
	"title": "Documentation resources",
	"tags": [],
	"description": "",
	"content": " 1. SageMaker SDK API guide https://sagemaker.readthedocs.io/en/stable/\n2. SageMaker Sample Notebooks on GitHub https://github.com/awslabs/amazon-sagemaker-examples\n3. SageMaker Developer Guide https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html\n4. SageMaker API Reference https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_Operations_Amazon_SageMaker_Service.html\n"
},
{
	"uri": "/",
	"title": "Welcome to Solving natural language processing problems with Amazon SageMaker Workshop at AMER Summit",
	"tags": [],
	"description": "",
	"content": " Solving natural language processing problems with Amazon SageMaker Workshop Presenters    Shashank Prasanna, Sr. Developer Advocate, AI/ML Amir Imani,\nAI/ML Specialist SA     Twitter: @shshnkp\nlinkedin.com/in/shashankprasanna\nmedium.com/@shashankprasanna Twitter: @amirhos_imani\n    Workshop Overview Workshop duration: 2 hours\nAbstract: Recent advances in machine learning (ML) have enabled significant breakthroughs in the field of natural language processing (NLP). With state-of-the-art ML models, you can perform a range of NLP tasks such as sentiment analysis, text summarization, and more. In this 2-hour, fully remote technical workshop, AWS experts walk you through step-by-step instructions on how to use Amazon SageMaker and Hugging Face to build, train, tune, and deploy ML models at scale.\nAt the end of this workshop you will be able to:\n Easily fine-tune or train NLP models from scratch on text data using Amazon SageMaker and HuggingFace library integration.\n Accelerate ML development and optimize training cost by leveraging Amazon SageMaker Studio notebooks, fully-managed training instances and managed spot training\n Apply the steps learnt in the workshop on your own datasets for NLP usecases such as text classification, text summarization Q\u0026amp;A, and others\n  Agenda    Topics Duration Presenter     Workshop overview and Getting started with Amazon SageMaker Studio 15 mins Shashank   TBD 30 mins Shashank   Break 15 mins Switch presenters   TBD 50 mins Amir   Wrap Up 10 mins Shashank    "
},
{
	"uri": "/0_introduction.html",
	"title": "Workshop Interface Overview",
	"tags": [],
	"description": "",
	"content": "Watch this short video at anytime to familiarize yourself with the workshop interface.   "
},
{
	"uri": "/1_setup.html",
	"title": "1. Getting started",
	"tags": [],
	"description": "",
	"content": " Section presenter: Shashank\n For this workshop you\u0026rsquo;ll get access to a temporary AWS Account already pre-configured with Amazon SageMaker Studio. Follow the steps in this section to login to your AWS Account and download the workshop material.\n"
},
{
	"uri": "/1_setup/download_workshop_content.html",
	"title": "1.2 Download workshop content",
	"tags": [],
	"description": "",
	"content": " Open a new terminal window Clone the workshop content In the terminal paste the following command to clone the workshop content repo:\ngit clone https://github.com/shashankprasanna/sagemaker-amer-summit-workshop.git  Double click on the sagemaker-amer-summit-workshop folder Double click on notebooks folder Double click on the the first notebook Choose the Python 3 (Data Science kernel) and hit select "
},
{
	"uri": "/4_clean-up/cleanup.html",
	"title": "Delete all resources",
	"tags": [],
	"description": "",
	"content": " This workshop creates the following resources:\n SageMaker Endpoints S3 objects SageMaker apps  If you completed section 3.1, the \u0026ldquo;Delete resources\u0026rdquo; section at the end deletes running SageMaker Endpoints and all S3 objects created during the workshop.\nYou can also delete the endpoints by navigating to AWS Console \u0026gt; Amazon SageMaker. In the left menu click on Inference \u0026gt; Endpoints. Select the endpoint you want to delete and click on Action \u0026gt; Delete.\nFor additional information about deleting SageMaker resources, please visit the following documentation page: https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html\nDeleting SageMaker Studio apps Using the AWS Console SageMaker Studio also creates apps such as JupyterServer (used to run notebooks), DataWrangler and Debugger which run on EC2 instances. Use the following instructions to shutdown running apps:\nNavigate to AWS Console \u0026gt; Amazon SageMaker \u0026gt; Amazon SageMaker Studio. This will open up the SageMaker Studio Control Panel. Click on the Studio user who’s resources you want to delete.\nUnder User Details click on “Delete app” to delete all running apps. Keep the “default” App if you want to continue working with SageMaker Studio and want to launch new notebooks.\nUsing the SageMaker Studio In SageMaker Studio Notebook, click on the running apps menu which is 3rd from the top. Click on all the power buttons to shut down apps. Keep the running instances if you want to continue working on SageMaker Notebook.\nFor more information about deleting Studio resources, Studio domain and how to delete resources using AWS CLI visit the following documentation page: https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-delete-domain.html?icmpid=docs_sagemaker_console_studio\n"
},
{
	"uri": "/3_build-train-tune-deploy.html",
	"title": "2. Amazon SageMaker and HuggingFace library",
	"tags": [],
	"description": "",
	"content": " Section presenter: Amir, Shashank\n In this section you\u0026rsquo;ll learn how to build train and tune your machine learning models. You will cover:  Preparing your training, validation and test sets Preparing a HuggingFace Transformers fine-tuning script Using Amazon SageMaker and HuggingFace to fine-tune models on your datasets Using Amazon SageMaker Debugger to debug and profile your training jobs Deploying NLP models to endpoints using SageMaker and evaluating them  "
},
{
	"uri": "/appendix/resources.html",
	"title": "Technical papers",
	"tags": [],
	"description": "",
	"content": " 1. Whitepaper on AI Fairness https://pages.awscloud.com/rs/112-TZM-766/images/Amazon.AI.Fairness.and.Explainability.Whitepaper.pdf 2. Paper on Debugging ML models https://www.amazon.science/publications/amazon-sagemaker-debugger-a-system-for-real-time-insights-into-machine-learning-model-training "
},
{
	"uri": "/appendix/blogposts_videos.html",
	"title": "Blogposts and videos",
	"tags": [],
	"description": "",
	"content": " 1. ML blog posts https://medium.com/@shashankprasanna 2. SageMaker Clarify Interview at re:invent 2020 with Principal Scientist at AWS AI   3. SageMaker Debugger Interview at re:invent 2020 with Principal PM at AWS AI   4. AWS Blog posts https://aws.amazon.com/blogs/machine-learning/ "
},
{
	"uri": "/4_clean-up.html",
	"title": "3. Clean up resources",
	"tags": [],
	"description": "",
	"content": "In this section you\u0026rsquo;ll find instructions to clean up resources used for this workshop.\n"
},
{
	"uri": "/appendix.html",
	"title": "Appendix",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]